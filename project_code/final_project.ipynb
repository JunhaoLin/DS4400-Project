{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4400 Final Project\n",
    "### Instructor: Ehsan Elhamifar\n",
    "### Team member: Junhao Lin, Minghua Zhang, Mengting Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "#Reading the data, using pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a dataset class\n",
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.tensor(x,dtype=torch.float32)\n",
    "    self.y = torch.tensor(y,dtype=torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ID', 'Age', 'Gender', 'Education', 'Country', 'Ethnicity', 'Neuroticism', 'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Impulsiveness', 'Sensation_seeking', 'Alcohol', 'Amphetamine', 'Amyl_nitrite', 'Benzodiazepine', 'Caffeine', 'Cannabis', 'Chocolate', 'Cocaine', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legal_highs', 'LSD', 'Methadone', 'Mushrooms', 'Nicotine', 'Semeron', 'VSA']\n",
    "data = pd.read_csv(\"drug_consumption.data\",header = None, names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>...</th>\n",
       "      <th>Ecstasy</th>\n",
       "      <th>Heroin</th>\n",
       "      <th>Ketamine</th>\n",
       "      <th>Legal_highs</th>\n",
       "      <th>LSD</th>\n",
       "      <th>Methadone</th>\n",
       "      <th>Mushrooms</th>\n",
       "      <th>Nicotine</th>\n",
       "      <th>Semeron</th>\n",
       "      <th>VSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>-0.05188</td>\n",
       "      <td>-1.76250</td>\n",
       "      <td>0.58331</td>\n",
       "      <td>-0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.78155</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-1.43719</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>1.49158</td>\n",
       "      <td>-1.92173</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-1.77200</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.05188</td>\n",
       "      <td>-1.76250</td>\n",
       "      <td>0.88309</td>\n",
       "      <td>-0.76096</td>\n",
       "      <td>2.33337</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.28519</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.79151</td>\n",
       "      <td>0.32197</td>\n",
       "      <td>0.29338</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>-0.27607</td>\n",
       "      <td>...</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-1.19430</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>1.88511</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.13788</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>0.58331</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.51840</td>\n",
       "      <td>...</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>1.13281</td>\n",
       "      <td>-1.37639</td>\n",
       "      <td>-1.27553</td>\n",
       "      <td>-1.77200</td>\n",
       "      <td>-1.38502</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.91093</td>\n",
       "      <td>-1.92173</td>\n",
       "      <td>0.29338</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-2.57309</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.21128</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>2.12700</td>\n",
       "      <td>1.65653</td>\n",
       "      <td>1.11406</td>\n",
       "      <td>0.41594</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age   Gender  Education  Country  Ethnicity  Neuroticism  \\\n",
       "ID                                                                   \n",
       "1879 -0.95197 -0.48246   -0.61113 -0.57009    0.12600     -0.05188   \n",
       "1880 -0.07854 -0.48246   -0.61113  0.24923    0.11440     -0.14882   \n",
       "1881 -0.95197 -0.48246   -1.43719 -0.57009   -0.31685      1.49158   \n",
       "1882 -0.95197 -0.48246    0.45468  0.24923   -0.31685     -0.05188   \n",
       "1883 -0.95197 -0.48246   -0.61113 -0.28519   -0.31685     -0.79151   \n",
       "1884 -0.95197  0.48246   -0.61113 -0.57009   -0.31685     -1.19430   \n",
       "1885 -0.95197 -0.48246   -0.61113 -0.57009   -0.31685     -0.24649   \n",
       "1886 -0.07854  0.48246    0.45468 -0.57009   -0.31685      1.13281   \n",
       "1887 -0.95197  0.48246   -0.61113 -0.57009   -0.31685      0.91093   \n",
       "1888 -0.95197 -0.48246   -0.61113  0.21128   -0.31685     -0.46725   \n",
       "\n",
       "      Extraversion  Openness  Agreeableness  Conscientiousness  ...  Ecstasy  \\\n",
       "ID                                                              ...            \n",
       "1879      -1.76250   0.58331       -0.76096           -0.14277  ...      CL5   \n",
       "1880      -0.57545   1.43533       -0.91699           -0.78155  ...      CL3   \n",
       "1881      -1.92173  -0.58331       -1.77200            0.58489  ...      CL2   \n",
       "1882      -1.76250   0.88309       -0.76096            2.33337  ...      CL0   \n",
       "1883       0.32197   0.29338       -0.30172           -0.27607  ...      CL5   \n",
       "1884       1.74091   1.88511        0.76096           -1.13788  ...      CL0   \n",
       "1885       1.74091   0.58331        0.76096           -1.51840  ...      CL2   \n",
       "1886      -1.37639  -1.27553       -1.77200           -1.38502  ...      CL4   \n",
       "1887      -1.92173   0.29338       -1.62090           -2.57309  ...      CL3   \n",
       "1888       2.12700   1.65653        1.11406            0.41594  ...      CL3   \n",
       "\n",
       "      Heroin Ketamine Legal_highs  LSD Methadone Mushrooms Nicotine Semeron  \\\n",
       "ID                                                                            \n",
       "1879     CL0      CL0         CL2  CL4       CL0       CL4      CL6     CL0   \n",
       "1880     CL0      CL3         CL5  CL3       CL0       CL4      CL2     CL0   \n",
       "1881     CL5      CL0         CL2  CL0       CL6       CL0      CL6     CL0   \n",
       "1882     CL0      CL0         CL2  CL0       CL0       CL2      CL2     CL0   \n",
       "1883     CL2      CL0         CL4  CL5       CL4       CL0      CL6     CL0   \n",
       "1884     CL0      CL0         CL3  CL3       CL0       CL0      CL0     CL0   \n",
       "1885     CL0      CL0         CL3  CL5       CL4       CL4      CL5     CL0   \n",
       "1886     CL0      CL2         CL0  CL2       CL0       CL2      CL6     CL0   \n",
       "1887     CL0      CL0         CL3  CL3       CL0       CL3      CL4     CL0   \n",
       "1888     CL0      CL0         CL3  CL3       CL0       CL3      CL6     CL0   \n",
       "\n",
       "      VSA  \n",
       "ID         \n",
       "1879  CL2  \n",
       "1880  CL2  \n",
       "1881  CL2  \n",
       "1882  CL0  \n",
       "1883  CL1  \n",
       "1884  CL5  \n",
       "1885  CL0  \n",
       "1886  CL0  \n",
       "1887  CL0  \n",
       "1888  CL2  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the 'ID' as the index of the data \n",
    "data.set_index('ID', inplace = True)\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1       0.49788\n",
       "2      -0.07854\n",
       "3       0.49788\n",
       "4      -0.95197\n",
       "5       0.49788\n",
       "         ...   \n",
       "1884   -0.95197\n",
       "1885   -0.95197\n",
       "1886   -0.07854\n",
       "1887   -0.95197\n",
       "1888   -0.95197\n",
       "Name: Age, Length: 1885, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the age, gender, education, country, and ethnicity columns in the data have been encoded when we extracted them. \n",
    "In order to put them in use, we have to decode them acoording to the instruction provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also need to convert the digits of these values into five decimal places since the orginal ones' is float64 \n",
    "#If we kept float64, the comparsion would not work \n",
    "\n",
    "for term in ['Age', 'Gender', 'Education', 'Country', 'Ethnicity', 'Neuroticism',\n",
    "       'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness',\n",
    "       'Impulsiveness', 'Sensation_seeking']:\n",
    "    for value in data[term]:\n",
    "        value = '{:.5f}'.format(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>...</th>\n",
       "      <th>Ecstasy</th>\n",
       "      <th>Heroin</th>\n",
       "      <th>Ketamine</th>\n",
       "      <th>Legal_highs</th>\n",
       "      <th>LSD</th>\n",
       "      <th>Methadone</th>\n",
       "      <th>Mushrooms</th>\n",
       "      <th>Nicotine</th>\n",
       "      <th>Semeron</th>\n",
       "      <th>VSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-1.19430</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>1.88511</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.13788</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>0.58331</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.51840</td>\n",
       "      <td>...</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>1.13281</td>\n",
       "      <td>-1.37639</td>\n",
       "      <td>-1.27553</td>\n",
       "      <td>-1.77200</td>\n",
       "      <td>-1.38502</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.91093</td>\n",
       "      <td>-1.92173</td>\n",
       "      <td>0.29338</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-2.57309</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.21128</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>2.12700</td>\n",
       "      <td>1.65653</td>\n",
       "      <td>1.11406</td>\n",
       "      <td>0.41594</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age   Gender  Education  Country  Ethnicity  Neuroticism  \\\n",
       "ID                                                                   \n",
       "1     0.49788  0.48246   -0.05921  0.96082    0.12600      0.31287   \n",
       "2    -0.07854 -0.48246    1.98437  0.96082   -0.31685     -0.67825   \n",
       "3     0.49788 -0.48246   -0.05921  0.96082   -0.31685     -0.46725   \n",
       "4    -0.95197  0.48246    1.16365  0.96082   -0.31685     -0.14882   \n",
       "5     0.49788  0.48246    1.98437  0.96082   -0.31685      0.73545   \n",
       "...       ...      ...        ...      ...        ...          ...   \n",
       "1884 -0.95197  0.48246   -0.61113 -0.57009   -0.31685     -1.19430   \n",
       "1885 -0.95197 -0.48246   -0.61113 -0.57009   -0.31685     -0.24649   \n",
       "1886 -0.07854  0.48246    0.45468 -0.57009   -0.31685      1.13281   \n",
       "1887 -0.95197  0.48246   -0.61113 -0.57009   -0.31685      0.91093   \n",
       "1888 -0.95197 -0.48246   -0.61113  0.21128   -0.31685     -0.46725   \n",
       "\n",
       "      Extraversion  Openness  Agreeableness  Conscientiousness  ...  Ecstasy  \\\n",
       "ID                                                              ...            \n",
       "1         -0.57545  -0.58331       -0.91699           -0.00665  ...      CL0   \n",
       "2          1.93886   1.43533        0.76096           -0.14277  ...      CL4   \n",
       "3          0.80523  -0.84732       -1.62090           -1.01450  ...      CL0   \n",
       "4         -0.80615  -0.01928        0.59042            0.58489  ...      CL0   \n",
       "5         -1.63340  -0.45174       -0.30172            1.30612  ...      CL1   \n",
       "...            ...       ...            ...                ...  ...      ...   \n",
       "1884       1.74091   1.88511        0.76096           -1.13788  ...      CL0   \n",
       "1885       1.74091   0.58331        0.76096           -1.51840  ...      CL2   \n",
       "1886      -1.37639  -1.27553       -1.77200           -1.38502  ...      CL4   \n",
       "1887      -1.92173   0.29338       -1.62090           -2.57309  ...      CL3   \n",
       "1888       2.12700   1.65653        1.11406            0.41594  ...      CL3   \n",
       "\n",
       "      Heroin Ketamine Legal_highs  LSD Methadone Mushrooms Nicotine Semeron  \\\n",
       "ID                                                                            \n",
       "1        CL0      CL0         CL0  CL0       CL0       CL0      CL2     CL0   \n",
       "2        CL0      CL2         CL0  CL2       CL3       CL0      CL4     CL0   \n",
       "3        CL0      CL0         CL0  CL0       CL0       CL1      CL0     CL0   \n",
       "4        CL0      CL2         CL0  CL0       CL0       CL0      CL2     CL0   \n",
       "5        CL0      CL0         CL1  CL0       CL0       CL2      CL2     CL0   \n",
       "...      ...      ...         ...  ...       ...       ...      ...     ...   \n",
       "1884     CL0      CL0         CL3  CL3       CL0       CL0      CL0     CL0   \n",
       "1885     CL0      CL0         CL3  CL5       CL4       CL4      CL5     CL0   \n",
       "1886     CL0      CL2         CL0  CL2       CL0       CL2      CL6     CL0   \n",
       "1887     CL0      CL0         CL3  CL3       CL0       CL3      CL4     CL0   \n",
       "1888     CL0      CL0         CL3  CL3       CL0       CL3      CL6     CL0   \n",
       "\n",
       "      VSA  \n",
       "ID         \n",
       "1     CL0  \n",
       "2     CL0  \n",
       "3     CL0  \n",
       "4     CL0  \n",
       "5     CL0  \n",
       "...   ...  \n",
       "1884  CL5  \n",
       "1885  CL0  \n",
       "1886  CL0  \n",
       "1887  CL0  \n",
       "1888  CL2  \n",
       "\n",
       "[1885 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to manipulate data and transform them into more understandable formats\n",
    "\n",
    "def decodeAge(age) -> int:\n",
    "    \"\"\"We need to convert age into reasonable and intuitive catagories:\n",
    "        - 0: 18-24\n",
    "        - 1: 25-34\n",
    "        - 2: 35-44\n",
    "        - 3: 45-54\n",
    "        - 4: 55-64\n",
    "        - 5: 65+\n",
    "    \"\"\"\n",
    "    if (age == -0.95197):\n",
    "        age = 0\n",
    "    elif (age == -0.07854):\n",
    "        age = 1\n",
    "    elif (age == 0.49788):\n",
    "        age = 2\n",
    "    elif (age == 1.09449):\n",
    "        age = 3\n",
    "    elif (age == 1.82213):\n",
    "        age = 4\n",
    "    elif (age == 2.59171):\n",
    "        age = 5\n",
    "    return age\n",
    "\n",
    "data['Age'] = data['Age'].map(decodeAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeGender(gender) -> int:\n",
    "    \"\"\"Transform gender into understandable format\n",
    "        - 0: Male\n",
    "        - 1: Female\n",
    "    \"\"\"\n",
    "    if (gender == 0.48246):\n",
    "        gender = 1\n",
    "    elif (gender == -0.48246 ):\n",
    "        gender = 0\n",
    "    return gender\n",
    "\n",
    "data['Gender'] = data['Gender'].map(decodeGender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeEducation(edu) -> int:\n",
    "    \"\"\"Transform education level into understandable format\n",
    "        - 0: Left school before 16 years\n",
    "        - 1: Left school at 16 years\n",
    "        - 2: Left school at 17 years\n",
    "        - 3: Left school at 18 years \n",
    "        - 4: Some college or university, no certificate or degree \n",
    "        - 5: Professional certificate/ diploma   \n",
    "        - 6: University degree \n",
    "        - 7: Masters degree \n",
    "        - 8: Doctorate degree\n",
    "\n",
    "    \"\"\"\n",
    "    if (edu == -2.43591):\n",
    "        edu = 0\n",
    "    elif (edu == -1.73790):\n",
    "        edu = 1\n",
    "    elif (edu == -1.43719):\n",
    "        edu = 2\n",
    "    elif (edu == -1.22751):\n",
    "        edu = 3\n",
    "    elif (edu == -0.61113):\n",
    "        edu = 4\n",
    "    elif (edu == -0.05921):\n",
    "        edu = 5\n",
    "    elif (edu == 0.45468):\n",
    "        edu = 6\n",
    "    elif (edu == 1.16365):\n",
    "        edu = 7\n",
    "    elif (edu == 1.98437):\n",
    "        edu = 8\n",
    "    return edu\n",
    "\n",
    "data['Education'] = data['Education'].map(decodeEducation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeCountry(country): \n",
    "    \"\"\"Transform country of origin into understandable format\n",
    "        - 0: Australia\n",
    "        - 1: Canada\n",
    "        - 2: New Zealand\n",
    "        - 3: Other\n",
    "        - 4: Republic of Ireland\n",
    "        - 5: UK\n",
    "        - 6: USA\n",
    "    \"\"\" \n",
    "    if (country == -0.09765):\n",
    "        country = 0\n",
    "    elif (country == 0.24923):\n",
    "        country = 1\n",
    "    elif (country == -0.46841):\n",
    "        country= 2\n",
    "    elif (country == -0.28519):\n",
    "        country = 3\n",
    "    elif (country == 0.21128):\n",
    "        country = 4\n",
    "    elif (country == 0.96082):\n",
    "        country = 5\n",
    "    elif (country == -0.57009):\n",
    "        country = 6\n",
    "    return country\n",
    "\n",
    "data['Country'] = data['Country'].map(decodeCountry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeEthnicity(eth):\n",
    "    \"\"\"Decode ethnicities into understandable catagories\n",
    "        - 0: Asian\n",
    "        - 1: Black\n",
    "        - 2: Mixed-Black/Asian\n",
    "        - 3: Mixed-White/Asian\n",
    "        - 4: Mixed-White/Black \n",
    "        - 5: Other\n",
    "        - 6: White\n",
    "    \"\"\"\n",
    "    if (eth == -0.50212):\n",
    "        eth = 0\n",
    "    elif (eth == -1.10702):\n",
    "        eth = 1\n",
    "    elif (eth == 1.90725):\n",
    "        eth = 2\n",
    "    elif (eth == 0.12600):\n",
    "        eth = 3\n",
    "    elif (eth == -0.22166):\n",
    "        eth = 4\n",
    "    elif (eth == 0.11440):\n",
    "        eth = 5\n",
    "    elif (eth == -0.31685):\n",
    "        eth = 6\n",
    "    return eth\n",
    "\n",
    "data['Ethnicity'] = data['Ethnicity'].map(decodeEthnicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>...</th>\n",
       "      <th>Ecstasy</th>\n",
       "      <th>Heroin</th>\n",
       "      <th>Ketamine</th>\n",
       "      <th>Legal_highs</th>\n",
       "      <th>LSD</th>\n",
       "      <th>Methadone</th>\n",
       "      <th>Mushrooms</th>\n",
       "      <th>Nicotine</th>\n",
       "      <th>Semeron</th>\n",
       "      <th>VSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.19430</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>1.88511</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.13788</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>0.58331</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.51840</td>\n",
       "      <td>...</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.13281</td>\n",
       "      <td>-1.37639</td>\n",
       "      <td>-1.27553</td>\n",
       "      <td>-1.77200</td>\n",
       "      <td>-1.38502</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.91093</td>\n",
       "      <td>-1.92173</td>\n",
       "      <td>0.29338</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-2.57309</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.21128</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>2.12700</td>\n",
       "      <td>1.65653</td>\n",
       "      <td>1.11406</td>\n",
       "      <td>0.41594</td>\n",
       "      <td>...</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age   Gender  Education  Country  Ethnicity  Neuroticism  \\\n",
       "ID                                                                   \n",
       "1     0.49788  0.48246   -0.05921  0.96082        3.0      0.31287   \n",
       "2     1.00000 -0.48246    8.00000  0.96082        6.0     -0.67825   \n",
       "3     0.49788 -0.48246   -0.05921  0.96082        6.0     -0.46725   \n",
       "4    -0.95197  0.48246    7.00000  0.96082        6.0     -0.14882   \n",
       "5     0.49788  0.48246    8.00000  0.96082        6.0      0.73545   \n",
       "...       ...      ...        ...      ...        ...          ...   \n",
       "1884 -0.95197  0.48246   -0.61113  6.00000        6.0     -1.19430   \n",
       "1885 -0.95197 -0.48246   -0.61113  6.00000        6.0     -0.24649   \n",
       "1886  1.00000  0.48246    0.45468  6.00000        6.0      1.13281   \n",
       "1887 -0.95197  0.48246   -0.61113  6.00000        6.0      0.91093   \n",
       "1888 -0.95197 -0.48246   -0.61113  0.21128        6.0     -0.46725   \n",
       "\n",
       "      Extraversion  Openness  Agreeableness  Conscientiousness  ...  Ecstasy  \\\n",
       "ID                                                              ...            \n",
       "1         -0.57545  -0.58331       -0.91699           -0.00665  ...      CL0   \n",
       "2          1.93886   1.43533        0.76096           -0.14277  ...      CL4   \n",
       "3          0.80523  -0.84732       -1.62090           -1.01450  ...      CL0   \n",
       "4         -0.80615  -0.01928        0.59042            0.58489  ...      CL0   \n",
       "5         -1.63340  -0.45174       -0.30172            1.30612  ...      CL1   \n",
       "...            ...       ...            ...                ...  ...      ...   \n",
       "1884       1.74091   1.88511        0.76096           -1.13788  ...      CL0   \n",
       "1885       1.74091   0.58331        0.76096           -1.51840  ...      CL2   \n",
       "1886      -1.37639  -1.27553       -1.77200           -1.38502  ...      CL4   \n",
       "1887      -1.92173   0.29338       -1.62090           -2.57309  ...      CL3   \n",
       "1888       2.12700   1.65653        1.11406            0.41594  ...      CL3   \n",
       "\n",
       "      Heroin Ketamine Legal_highs  LSD Methadone Mushrooms Nicotine Semeron  \\\n",
       "ID                                                                            \n",
       "1        CL0      CL0         CL0  CL0       CL0       CL0      CL2     CL0   \n",
       "2        CL0      CL2         CL0  CL2       CL3       CL0      CL4     CL0   \n",
       "3        CL0      CL0         CL0  CL0       CL0       CL1      CL0     CL0   \n",
       "4        CL0      CL2         CL0  CL0       CL0       CL0      CL2     CL0   \n",
       "5        CL0      CL0         CL1  CL0       CL0       CL2      CL2     CL0   \n",
       "...      ...      ...         ...  ...       ...       ...      ...     ...   \n",
       "1884     CL0      CL0         CL3  CL3       CL0       CL0      CL0     CL0   \n",
       "1885     CL0      CL0         CL3  CL5       CL4       CL4      CL5     CL0   \n",
       "1886     CL0      CL2         CL0  CL2       CL0       CL2      CL6     CL0   \n",
       "1887     CL0      CL0         CL3  CL3       CL0       CL3      CL4     CL0   \n",
       "1888     CL0      CL0         CL3  CL3       CL0       CL3      CL6     CL0   \n",
       "\n",
       "      VSA  \n",
       "ID         \n",
       "1     CL0  \n",
       "2     CL0  \n",
       "3     CL0  \n",
       "4     CL0  \n",
       "5     CL0  \n",
       "...   ...  \n",
       "1884  CL5  \n",
       "1885  CL0  \n",
       "1886  CL0  \n",
       "1887  CL0  \n",
       "1888  CL2  \n",
       "\n",
       "[1885 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we want binary classification, we want to modify orginal data: \n",
    "\n",
    "#### when a person consumed the drug last year, last decade, over a decade ago or never used before, we classifed them as 'non-user' and label them as --  -1 \n",
    "\n",
    "#### otherwise, when a person consumed the drug last month, last week, or last day, we considered them as 'user' of this drug -- +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyFre(a):\n",
    "    if ((a == 'CL6') or (a == 'CL5') or (a == 'CL4') ):\n",
    "        a = 1\n",
    "    elif ((a == 'CL0') or (a == 'CL1') or (a == 'CL2') or (a == 'CL3')):\n",
    "        a = -1\n",
    "    \n",
    "    return a\n",
    "\n",
    "for drug in ['Alcohol', 'Amphetamine',\n",
    "       'Amyl_nitrite', 'Benzodiazepine', 'Caffeine', 'Cannabis', 'Chocolate',\n",
    "       'Cocaine', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legal_highs',\n",
    "       'LSD', 'Methadone', 'Mushrooms', 'Nicotine', 'Semeron', 'VSA']:\n",
    "    data[drug] = data[drug].map(classifyFre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also classify 18 drugs into three major categories in order to simpify the problem:\n",
    "#notice: some drugs belong to \n",
    "#Heroin, Ecstasy, and Benzodiazepines\n",
    "\n",
    "data['Heroins'] = data.apply(lambda x: int((x['Cocaine'] + x['Crack'] + x['Heroin'] + x['Methadone'])>-4), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ecstasies'] = data.apply(lambda x: int((x['Amphetamine']  + x['Cannabis'] + x['Cocaine']  + x['Ecstasy'] + x['Ketamine'] + x['LSD'] + x['Methadone'] + x['Mushrooms'] )>-8), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['Benzodiazepines'] = data.apply(lambda x: int((x['Amphetamine'] + x['Cocaine'] + x['Methadone'])>-3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Caffeine','Chocolate','Nicotine','Legal_highs','Alcohol','Amphetamine','Amyl_nitrite','Benzodiazepine', 'Cannabis', 'Cocaine', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'LSD', 'Methadone', 'Mushrooms', 'Semeron', 'VSA'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Impulsiveness</th>\n",
       "      <th>Sensation_seeking</th>\n",
       "      <th>Heroins</th>\n",
       "      <th>Ecstasies</th>\n",
       "      <th>Benzodiazepines</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>0.40148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age   Gender  Education  Country  Ethnicity  Neuroticism  \\\n",
       "ID                                                                 \n",
       "1   0.49788  0.48246   -0.05921  0.96082        3.0      0.31287   \n",
       "2   1.00000 -0.48246    8.00000  0.96082        6.0     -0.67825   \n",
       "3   0.49788 -0.48246   -0.05921  0.96082        6.0     -0.46725   \n",
       "4  -0.95197  0.48246    7.00000  0.96082        6.0     -0.14882   \n",
       "5   0.49788  0.48246    8.00000  0.96082        6.0      0.73545   \n",
       "\n",
       "    Extraversion  Openness  Agreeableness  Conscientiousness  Impulsiveness  \\\n",
       "ID                                                                            \n",
       "1       -0.57545  -0.58331       -0.91699           -0.00665       -0.21712   \n",
       "2        1.93886   1.43533        0.76096           -0.14277       -0.71126   \n",
       "3        0.80523  -0.84732       -1.62090           -1.01450       -1.37983   \n",
       "4       -0.80615  -0.01928        0.59042            0.58489       -1.37983   \n",
       "5       -1.63340  -0.45174       -0.30172            1.30612       -0.21712   \n",
       "\n",
       "    Sensation_seeking  Heroins  Ecstasies  Benzodiazepines  \n",
       "ID                                                          \n",
       "1            -1.18084        0          0                0  \n",
       "2            -0.21575        0          1                0  \n",
       "3             0.40148        0          0                0  \n",
       "4            -1.18084        0          0                0  \n",
       "5            -0.21575        0          0                0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also split the dataset into three parts based on three targets:\n",
    "\n",
    "data1 = data.drop(['Ecstasies', 'Benzodiazepines'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['Heroins', 'Benzodiazepines'], axis = 1)\n",
    "data3 = data.drop(['Ecstasies', 'Heroins'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into features and target \n",
    "y1 = data1['Heroins']\n",
    "X1 = data1.drop(['Heroins'], axis = 1)\n",
    "\n",
    "y2 = data2['Ecstasies']\n",
    "X2 = data2.drop(['Ecstasies'], axis = 1)\n",
    "\n",
    "y3 = data3['Benzodiazepines']\n",
    "X3 = data3.drop(['Benzodiazepines'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, train_size = 0.8, random_state = 42)\n",
    "#X1_val, X1_test, y1_val, y1_test = train_test_split(X1_rem, y1_rem, train_size = 0.5, random_state = 42)\n",
    "\n",
    "# spliting the data on target ecstasyPl into train and test\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, train_size = 0.8, random_state = 42)\n",
    "#X2_val, X2_test, y2_val, y2_test = train_test_split(X2_rem, y2_rem, train_size = 0.5, random_state = 42)\n",
    "\n",
    "# spliting the data on target benzoPl into train and test\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, train_size = 0.8, random_state = 42)\n",
    "#X3_val, X3_test, y3_val, y3_test = train_test_split(X3_rem, y3_rem, train_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function that train the model by give train data and return the f1 scores for both train set and test set\n",
    "from sklearn import metrics\n",
    "def trainAndEval(model, X_train, y_train, X_test, y_test): \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    test_score = metrics.f1_score(y_test, y_test_pred)\n",
    "    train_score = metrics.f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    return test_score, train_score\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Heroins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "clf = LogisticRegression(penalty = 'l2', C=0.01, solver = 'liblinear', class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.4214285714285714\n",
      "test:  0.4390243902439025\n"
     ]
    }
   ],
   "source": [
    "scores = trainAndEval(clf, X1_train, y1_train, X1_test, y1_test)\n",
    "print('train: ', scores[1])\n",
    "print('test: ', scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1e-20, 1e-19, 1e-18, 1e-17, 1e-16, 1e-15, 1e-14,\n",
       "                                1e-13, 1e-12, 1e-11, 1e-10, 1e-09, 1e-08, 1e-07,\n",
       "                                1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n",
       "                                100, 1000, 10000, 100000, 1000000],\n",
       "                          'class_weight': ['balanced'], 'penalty': ['l1', 'l2'],\n",
       "                          'solver': ['liblinear']}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "    \n",
    "    {\n",
    "     'penalty' : ['l1', 'l2'],\n",
    "    'C' : [10**i for i in range(-20,7)],\n",
    "    'solver' : ['liblinear'],\n",
    "     'class_weight': ['balanced']\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# Finding the best parameters by using Grid Search and cross validation \n",
    "\n",
    "scv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "heroin_GSCV = GridSearchCV(LogisticRegression(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_heroin_clf = heroin_GSCV.fit(X1_train, y1_train)\n",
    "\n",
    "best_heroin_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001,\n",
       " 'class_weight': 'balanced',\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_heroin_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.4368932038834951\n",
      "Train Score:  0.423444976076555\n"
     ]
    }
   ],
   "source": [
    "y1_pred_test = best_heroin_clf.best_estimator_.predict(X1_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y1_test, y1_pred_test))\n",
    "y1_pred_train = best_heroin_clf.best_estimator_.predict(X1_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y1_train, y1_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecstasies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1e-20, 1e-19, 1e-18, 1e-17, 1e-16, 1e-15, 1e-14,\n",
       "                                1e-13, 1e-12, 1e-11, 1e-10, 1e-09, 1e-08, 1e-07,\n",
       "                                1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n",
       "                                100, 1000, 10000, 100000, 1000000],\n",
       "                          'class_weight': ['balanced'], 'penalty': ['l1', 'l2'],\n",
       "                          'solver': ['liblinear']}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecstasy_GSCV = GridSearchCV(LogisticRegression(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_ecstasy_clf = ecstasy_GSCV.fit(X2_train, y2_train)\n",
    "\n",
    "best_ecstasy_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ecstasy_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.8131868131868132\n",
      "Train Score:  0.8126259234385492\n"
     ]
    }
   ],
   "source": [
    "y2_pred_test = best_ecstasy_clf.best_estimator_.predict(X2_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y2_test, y2_pred_test))\n",
    "\n",
    "y2_pred_train = best_ecstasy_clf.best_estimator_.predict(X2_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y2_train, y2_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benzos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1e-20, 1e-19, 1e-18, 1e-17, 1e-16, 1e-15, 1e-14,\n",
       "                                1e-13, 1e-12, 1e-11, 1e-10, 1e-09, 1e-08, 1e-07,\n",
       "                                1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n",
       "                                100, 1000, 10000, 100000, 1000000],\n",
       "                          'class_weight': ['balanced'], 'penalty': ['l1', 'l2'],\n",
       "                          'solver': ['liblinear']}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benzo_GSCV = GridSearchCV(LogisticRegression(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_benzo_clf = benzo_GSCV.fit(X3_train, y3_train)\n",
    "\n",
    "best_benzo_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_benzo_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.5302325581395348\n",
      "Train Score:  0.522279792746114\n"
     ]
    }
   ],
   "source": [
    "y3_pred_test = best_benzo_clf.best_estimator_.predict(X3_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y3_test, y3_pred_test))\n",
    "\n",
    "y3_pred_train = best_benzo_clf.best_estimator_.predict(X3_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y3_train, y3_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine \n",
    "## Heroins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  40 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [0, 0.5, 1, 10], 'kernel': ['linear', 'rbf'],\n",
       "                          'random_state': [0]}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "    \n",
    "    {\n",
    "     'kernel':['linear','rbf'],\n",
    "    'C' : [0,0.5,1,10],\n",
    "        'random_state': [0]\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# Finding the best parameters by using Grid Search and cross validation \n",
    "\n",
    "scv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "heroin_GSCV = GridSearchCV(SVC(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_heroin_clf = heroin_GSCV.fit(X1_train, y1_train)\n",
    "\n",
    "best_heroin_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'kernel': 'rbf', 'random_state': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_heroin_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.13114754098360656\n",
      "Train Score:  0.13284132841328414\n"
     ]
    }
   ],
   "source": [
    "y1_pred_test = best_heroin_clf.best_estimator_.predict(X1_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y1_test, y1_pred_test))\n",
    "y1_pred_train = best_heroin_clf.best_estimator_.predict(X1_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y1_train, y1_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecstasies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [0, 0.5, 1, 10], 'kernel': ['linear', 'rbf'],\n",
       "                          'random_state': [0]}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecstasy_GSCV = GridSearchCV(SVC(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_ecstasy_clf = ecstasy_GSCV.fit(X2_train, y2_train)\n",
    "\n",
    "best_ecstasy_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'rbf', 'random_state': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ecstasy_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.8181818181818181\n",
      "Train Score:  0.820754716981132\n"
     ]
    }
   ],
   "source": [
    "y2_pred_test = best_ecstasy_clf.best_estimator_.predict(X2_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y2_test, y2_pred_test))\n",
    "\n",
    "y2_pred_train = best_ecstasy_clf.best_estimator_.predict(X2_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y2_train, y2_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [0, 0.5, 1, 10], 'kernel': ['linear', 'rbf'],\n",
       "                          'random_state': [0]}],\n",
       "             scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benzo_GSCV = GridSearchCV(SVC(), param_grid = param_grid, scoring = 'f1', cv = scv, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_benzo_clf = benzo_GSCV.fit(X3_train, y3_train)\n",
    "\n",
    "best_benzo_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'kernel': 'rbf', 'random_state': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_benzo_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.3191489361702128\n",
      "Train Score:  0.42362525458248473\n"
     ]
    }
   ],
   "source": [
    "y3_pred_test = best_benzo_clf.best_estimator_.predict(X3_test)\n",
    "print(\"Test Score: \",metrics.f1_score(y3_test, y3_pred_test))\n",
    "\n",
    "y3_pred_train = best_benzo_clf.best_estimator_.predict(X3_train)\n",
    "print(\"Train Score: \",metrics.f1_score(y3_train, y3_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 500\n",
    "batch_size = 60\n",
    "\n",
    "# load training data\n",
    "trainset = dataset(X1_train.to_numpy(), y1_train.to_numpy())\n",
    "trainloader = DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# load testing data\n",
    "testset = dataset(X1_test.to_numpy(), y1_test.to_numpy())\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a configurable feed-forward network\n",
    "def feedforward_NN(num_input, num_output, num_layer, num_neuron, activation):\n",
    "    \"\"\"A function emcompassing a customizable Pytorch NN\n",
    "    \n",
    "     `Note: The activation function for output layer is always sigmoid function` \n",
    "\n",
    "        - num_input: The amount of input layers, usually the num of feature.\n",
    "        - num_output: The amount of output layers, usually 1 for binary classification\n",
    "        - num_layer: The amount of hidden layers.\n",
    "        - num_neuron: The amount of neuron in each hidden layer\n",
    "        - activation: The activation function to be used in every hidden layer \n",
    "    \"\"\"\n",
    "    if num_layer == 1:\n",
    "        if activation == 'sigmoid':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'tanh':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'identity':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Identity(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Identity(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'relu':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        else: raise RuntimeError('activation can only be one of the following: sigmoid, identity, relu, tanh')\n",
    "    elif num_layer == 2:\n",
    "        if activation == 'sigmoid':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'tanh':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'identity':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.Identity(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Identity(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.Identity(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        elif activation == 'relu':\n",
    "            model = nn.Sequential(nn.Linear(num_input, num_neuron),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(num_neuron, num_neuron),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(num_neuron, num_output),\n",
    "                                nn.Sigmoid())\n",
    "        else: raise RuntimeError('activation can only be one of the following: sigmoid, identity, relu, tanh')\n",
    "    else: raise RuntimeError('Expected number of layer to be either 1 or 2')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to train the model\n",
    "def train_NN(model, epochs, trainloader, optimizer, criterion, activation):\n",
    "  \"\"\"Method to train NN\"\"\"\n",
    "\n",
    "  def check_test_accuracy(y_test, y_pred) -> float:\n",
    "    \"\"\"helper method to determine the test accuracy by comparing test data and predicted data\"\"\"\n",
    "    counter = 0\n",
    "    for y1, y2 in zip(y_test, y_pred):\n",
    "        if y2 > 0.5 and y1 == 0:\n",
    "          counter += 1\n",
    "        if y2 <= 0.5 and y1 == 1:\n",
    "          counter += 1\n",
    "    \n",
    "    return (len(y_test) - counter) / len(y_pred)\n",
    "\n",
    "  print(f\"Current setting: activation function for hidden layer:{activation}, activation function for output layer: Sigmoid.\")\n",
    "\n",
    "  # Train data\n",
    "  # forward loop\n",
    "  for epoch in range(epochs):\n",
    "    # train data\n",
    "    print(f\"----------Epoch {epoch + 1}----------\")\n",
    "    running_loss = 0.0\n",
    "    for x_train, y_train in trainloader:\n",
    "      # Training pass\n",
    "      optimizer.zero_grad()\n",
    "      #print(y_train)\n",
    "      output = model(x_train)\n",
    "      #print(output)\n",
    "      loss = criterion(output, y_train.reshape(-1,1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "    else:\n",
    "      print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "    #test data\n",
    "    with torch.no_grad():\n",
    "      for x_test, y_test in testloader:\n",
    "        output2 = model(x_test)\n",
    "\n",
    "    # display training error\n",
    "    Training_accuracy = check_test_accuracy(y_train, output)\n",
    "    # displaing testing error\n",
    "    Testing_accuracy = check_test_accuracy(y_test, output2)\n",
    "    print(f\"Training error is {(1-Training_accuracy)*100}%\")\n",
    "    print(f\"Testing error is {(1-Testing_accuracy)}%\")\n",
    "\n",
    "  return model\n",
    "  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current setting: activation function for hidden layer:relu, activation function for output layer: Sigmoid.\n",
      "----------Epoch 1----------\n",
      "Training loss: 0.5848194177334125\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 2----------\n",
      "Training loss: 0.46278376418810624\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 3----------\n",
      "Training loss: 0.4240614754649309\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 4----------\n",
      "Training loss: 0.4152730955527379\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 5----------\n",
      "Training loss: 0.41458308467498195\n",
      "Training error is 37.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 6----------\n",
      "Training loss: 0.4020610371461281\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 7----------\n",
      "Training loss: 0.39378250332979053\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 8----------\n",
      "Training loss: 0.396144925401761\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 9----------\n",
      "Training loss: 0.3883126653157748\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 10----------\n",
      "Training loss: 0.3908488131486453\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 11----------\n",
      "Training loss: 0.3906467373554523\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 12----------\n",
      "Training loss: 0.3919457265963921\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 13----------\n",
      "Training loss: 0.3768397028056475\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 14----------\n",
      "Training loss: 0.39641613914416385\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 15----------\n",
      "Training loss: 0.38196577017123884\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 16----------\n",
      "Training loss: 0.3847187803341792\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 17----------\n",
      "Training loss: 0.38421191389744097\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 18----------\n",
      "Training loss: 0.38167818463765657\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 19----------\n",
      "Training loss: 0.3895038492404498\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 20----------\n",
      "Training loss: 0.3799786544763125\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 21----------\n",
      "Training loss: 0.37831005683312047\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 22----------\n",
      "Training loss: 0.3821549495825401\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 23----------\n",
      "Training loss: 0.3723870418392695\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 24----------\n",
      "Training loss: 0.3752804146363185\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 25----------\n",
      "Training loss: 0.37425471727664655\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 26----------\n",
      "Training loss: 0.36906757893470615\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 27----------\n",
      "Training loss: 0.3719717768522409\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 28----------\n",
      "Training loss: 0.378456365603667\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 29----------\n",
      "Training loss: 0.3818785192874762\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 30----------\n",
      "Training loss: 0.3772632365043347\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 31----------\n",
      "Training loss: 0.3833036428460708\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 32----------\n",
      "Training loss: 0.3729384701985579\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 33----------\n",
      "Training loss: 0.3660023831404172\n",
      "Training error is 0.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 34----------\n",
      "Training loss: 0.3820859807041975\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 35----------\n",
      "Training loss: 0.37613381216159236\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 36----------\n",
      "Training loss: 0.368083370419649\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 37----------\n",
      "Training loss: 0.36808537405270797\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 38----------\n",
      "Training loss: 0.37709057273773045\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 39----------\n",
      "Training loss: 0.3758027416009169\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 40----------\n",
      "Training loss: 0.3806407887202043\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 41----------\n",
      "Training loss: 0.3620559682066624\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 42----------\n",
      "Training loss: 0.3607985953298899\n",
      "Training error is 0.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 43----------\n",
      "Training loss: 0.37579106252927047\n",
      "Training error is 50.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 44----------\n",
      "Training loss: 0.3586425067713627\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 45----------\n",
      "Training loss: 0.37854300095484805\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 46----------\n",
      "Training loss: 0.3751685567773305\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 47----------\n",
      "Training loss: 0.35757493199064183\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 48----------\n",
      "Training loss: 0.35961246547790676\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 49----------\n",
      "Training loss: 0.3637762459424826\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 50----------\n",
      "Training loss: 0.3640569110329335\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 51----------\n",
      "Training loss: 0.35935164472231496\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 52----------\n",
      "Training loss: 0.3705361462556399\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 53----------\n",
      "Training loss: 0.3567797587468074\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 54----------\n",
      "Training loss: 0.3620953387939013\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 55----------\n",
      "Training loss: 0.3708712091812721\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 56----------\n",
      "Training loss: 0.35513816430018497\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 57----------\n",
      "Training loss: 0.3814485159057837\n",
      "Training error is 50.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 58----------\n",
      "Training loss: 0.3639785853716043\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 59----------\n",
      "Training loss: 0.3659791820324384\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 60----------\n",
      "Training loss: 0.36214548578629124\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 61----------\n",
      "Training loss: 0.3610154539346695\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 62----------\n",
      "Training loss: 0.3712990925862239\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 63----------\n",
      "Training loss: 0.371396504342556\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 64----------\n",
      "Training loss: 0.3581581740425183\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 65----------\n",
      "Training loss: 0.3528371166724425\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 66----------\n",
      "Training loss: 0.3673459910429441\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 67----------\n",
      "Training loss: 0.3512916633715996\n",
      "Training error is 0.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 68----------\n",
      "Training loss: 0.3659936659611188\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 69----------\n",
      "Training loss: 0.3653172008120097\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 70----------\n",
      "Training loss: 0.3597590464812059\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 71----------\n",
      "Training loss: 0.3471988213176911\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 72----------\n",
      "Training loss: 0.3631088011539899\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 73----------\n",
      "Training loss: 0.35173061146185947\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 74----------\n",
      "Training loss: 0.3473622563939828\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 75----------\n",
      "Training loss: 0.35610829637600827\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 76----------\n",
      "Training loss: 0.3729496391919943\n",
      "Training error is 37.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 77----------\n",
      "Training loss: 0.3649329508726413\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 78----------\n",
      "Training loss: 0.36010401122845137\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 79----------\n",
      "Training loss: 0.3457187654880377\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 80----------\n",
      "Training loss: 0.35563979412500674\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 81----------\n",
      "Training loss: 0.3534253549117308\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 82----------\n",
      "Training loss: 0.34564022490611446\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 83----------\n",
      "Training loss: 0.353290142921301\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 84----------\n",
      "Training loss: 0.3437735421153215\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 85----------\n",
      "Training loss: 0.3528261115917793\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 86----------\n",
      "Training loss: 0.34494376411804784\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 87----------\n",
      "Training loss: 0.3388923274782988\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 88----------\n",
      "Training loss: 0.33932851598812985\n",
      "Training error is 0.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 89----------\n",
      "Training loss: 0.3422177726259598\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 90----------\n",
      "Training loss: 0.34169141547038007\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 91----------\n",
      "Training loss: 0.34089649440004277\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 92----------\n",
      "Training loss: 0.35117286157149535\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 93----------\n",
      "Training loss: 0.35434023520121205\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 94----------\n",
      "Training loss: 0.3430032649865517\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 95----------\n",
      "Training loss: 0.3448239289797269\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 96----------\n",
      "Training loss: 0.3412568603570645\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 97----------\n",
      "Training loss: 0.34439527644560886\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 98----------\n",
      "Training loss: 0.3429870169896346\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 99----------\n",
      "Training loss: 0.35254570325979817\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 100----------\n",
      "Training loss: 0.3340874966233969\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 101----------\n",
      "Training loss: 0.3382916823029518\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 102----------\n",
      "Training loss: 0.3381439665189156\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 103----------\n",
      "Training loss: 0.3350197523832321\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 104----------\n",
      "Training loss: 0.3390219480945514\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 105----------\n",
      "Training loss: 0.3406833897416408\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 106----------\n",
      "Training loss: 0.3430738259966557\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 107----------\n",
      "Training loss: 0.35681117096772563\n",
      "Training error is 50.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 108----------\n",
      "Training loss: 0.3498378980618257\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 109----------\n",
      "Training loss: 0.3397848697809073\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 110----------\n",
      "Training loss: 0.3388224249848953\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 111----------\n",
      "Training loss: 0.34049645925943667\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 112----------\n",
      "Training loss: 0.3452877832146791\n",
      "Training error is 37.5%\n",
      "Testing error is 0.4117647058823529%\n",
      "----------Epoch 113----------\n",
      "Training loss: 0.3292270233997932\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 114----------\n",
      "Training loss: 0.3404234928580431\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 115----------\n",
      "Training loss: 0.34065660948936755\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 116----------\n",
      "Training loss: 0.34504136787011075\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 117----------\n",
      "Training loss: 0.3380244655104784\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 118----------\n",
      "Training loss: 0.33309968274373275\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 119----------\n",
      "Training loss: 0.32579075487760395\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 120----------\n",
      "Training loss: 0.3269445552275731\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 121----------\n",
      "Training loss: 0.330357001377986\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 122----------\n",
      "Training loss: 0.3385294217329759\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 123----------\n",
      "Training loss: 0.32860730301875335\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 124----------\n",
      "Training loss: 0.34000614056220424\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 125----------\n",
      "Training loss: 0.34519467101647305\n",
      "Training error is 25.0%\n",
      "Testing error is 0.47058823529411764%\n",
      "----------Epoch 126----------\n",
      "Training loss: 0.32766408301316774\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 127----------\n",
      "Training loss: 0.3313274515362886\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 128----------\n",
      "Training loss: 0.3353067504671904\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 129----------\n",
      "Training loss: 0.3378917700969256\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 130----------\n",
      "Training loss: 0.3322208237189513\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 131----------\n",
      "Training loss: 0.33255303708406597\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 132----------\n",
      "Training loss: 0.33198142510194045\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 133----------\n",
      "Training loss: 0.33386591420723843\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 134----------\n",
      "Training loss: 0.3265381633089139\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 135----------\n",
      "Training loss: 0.3246009023143695\n",
      "Training error is 0.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 136----------\n",
      "Training loss: 0.3282618746161461\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 137----------\n",
      "Training loss: 0.32657688626876247\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 138----------\n",
      "Training loss: 0.3232821489755924\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 139----------\n",
      "Training loss: 0.32964764478114933\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 140----------\n",
      "Training loss: 0.3189272447847403\n",
      "Training error is 0.0%\n",
      "Testing error is 0.47058823529411764%\n",
      "----------Epoch 141----------\n",
      "Training loss: 0.3277445991451924\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 142----------\n",
      "Training loss: 0.32274464575143963\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 143----------\n",
      "Training loss: 0.3220200068675555\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 144----------\n",
      "Training loss: 0.3260178714990616\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 145----------\n",
      "Training loss: 0.3240963105971997\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 146----------\n",
      "Training loss: 0.3178427735200295\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 147----------\n",
      "Training loss: 0.31607043227324116\n",
      "Training error is 0.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 148----------\n",
      "Training loss: 0.317986524448945\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 149----------\n",
      "Training loss: 0.32860879313487273\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 150----------\n",
      "Training loss: 0.3138298483995291\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 151----------\n",
      "Training loss: 0.3244207684810345\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 152----------\n",
      "Training loss: 0.3139051617338107\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 153----------\n",
      "Training loss: 0.3134495656077678\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 154----------\n",
      "Training loss: 0.32268631343658155\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 155----------\n",
      "Training loss: 0.32069558191757935\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 156----------\n",
      "Training loss: 0.3135290856544788\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 157----------\n",
      "Training loss: 0.317589809688238\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 158----------\n",
      "Training loss: 0.32205287596354115\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 159----------\n",
      "Training loss: 0.31813165430839246\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 160----------\n",
      "Training loss: 0.3105507745192601\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 161----------\n",
      "Training loss: 0.3174931269425612\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 162----------\n",
      "Training loss: 0.3214969136393987\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 163----------\n",
      "Training loss: 0.31265650001856\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 164----------\n",
      "Training loss: 0.31325615827853864\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 165----------\n",
      "Training loss: 0.31268098721137416\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 166----------\n",
      "Training loss: 0.316876650429689\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 167----------\n",
      "Training loss: 0.3224536581681325\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 168----------\n",
      "Training loss: 0.3117283399288471\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 169----------\n",
      "Training loss: 0.319016915674393\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 170----------\n",
      "Training loss: 0.31746700463386684\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 171----------\n",
      "Training loss: 0.32077800654447997\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 172----------\n",
      "Training loss: 0.3245290019191228\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 173----------\n",
      "Training loss: 0.3066904519039851\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 174----------\n",
      "Training loss: 0.3111761653652558\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 175----------\n",
      "Training loss: 0.3099892047735361\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 176----------\n",
      "Training loss: 0.3079741425239123\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 177----------\n",
      "Training loss: 0.3105992560203259\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 178----------\n",
      "Training loss: 0.3113285400546514\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 179----------\n",
      "Training loss: 0.30708714861136216\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 180----------\n",
      "Training loss: 0.31801652679076564\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 181----------\n",
      "Training loss: 0.3071891436210045\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 182----------\n",
      "Training loss: 0.3070022085538277\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 183----------\n",
      "Training loss: 0.30900265792241466\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 184----------\n",
      "Training loss: 0.3066354835262665\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 185----------\n",
      "Training loss: 0.3115620097288719\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 186----------\n",
      "Training loss: 0.3154750896187929\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 187----------\n",
      "Training loss: 0.3073874626022119\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 188----------\n",
      "Training loss: 0.3157551861726321\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 189----------\n",
      "Training loss: 0.30991509327521694\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 190----------\n",
      "Training loss: 0.3082128723080342\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 191----------\n",
      "Training loss: 0.30971440042440707\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 192----------\n",
      "Training loss: 0.3180662594162501\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 193----------\n",
      "Training loss: 0.31080624346549696\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 194----------\n",
      "Training loss: 0.3137982894594853\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 195----------\n",
      "Training loss: 0.3065281624977405\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 196----------\n",
      "Training loss: 0.3147402996054062\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 197----------\n",
      "Training loss: 0.30347060297544187\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 198----------\n",
      "Training loss: 0.31127723994163364\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 199----------\n",
      "Training loss: 0.30482509445685607\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 200----------\n",
      "Training loss: 0.2985687046669997\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 201----------\n",
      "Training loss: 0.3019030552643996\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 202----------\n",
      "Training loss: 0.29858776525809216\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 203----------\n",
      "Training loss: 0.3013138197935544\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 204----------\n",
      "Training loss: 0.302875405893876\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 205----------\n",
      "Training loss: 0.3032386331604077\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 206----------\n",
      "Training loss: 0.29588720116477746\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 207----------\n",
      "Training loss: 0.3014184024471503\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 208----------\n",
      "Training loss: 0.3098514968386063\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 209----------\n",
      "Training loss: 0.3004529613714952\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 210----------\n",
      "Training loss: 0.3039280279324605\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 211----------\n",
      "Training loss: 0.2921745272783133\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 212----------\n",
      "Training loss: 0.2955281533873998\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 213----------\n",
      "Training loss: 0.3034053321641225\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 214----------\n",
      "Training loss: 0.30077023116441876\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 215----------\n",
      "Training loss: 0.29883020428510815\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 216----------\n",
      "Training loss: 0.30734244791361004\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 217----------\n",
      "Training loss: 0.2939434183331636\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 218----------\n",
      "Training loss: 0.29899512231349945\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 219----------\n",
      "Training loss: 0.2978144001502257\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 220----------\n",
      "Training loss: 0.3002896159887314\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 221----------\n",
      "Training loss: 0.299425245477603\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 222----------\n",
      "Training loss: 0.3036761765296643\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 223----------\n",
      "Training loss: 0.29413512406440884\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 224----------\n",
      "Training loss: 0.29167308199864167\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 225----------\n",
      "Training loss: 0.2969057330718407\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 226----------\n",
      "Training loss: 0.29240530729293823\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 227----------\n",
      "Training loss: 0.2989664960366029\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 228----------\n",
      "Training loss: 0.2961810208283938\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 229----------\n",
      "Training loss: 0.31251713461600816\n",
      "Training error is 50.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 230----------\n",
      "Training loss: 0.3010478369318522\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 231----------\n",
      "Training loss: 0.3129116084713202\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 232----------\n",
      "Training loss: 0.299322526042278\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 233----------\n",
      "Training loss: 0.29322999945053685\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 234----------\n",
      "Training loss: 0.2920767108981426\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 235----------\n",
      "Training loss: 0.2998748908822353\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 236----------\n",
      "Training loss: 0.29924822541383594\n",
      "Training error is 37.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 237----------\n",
      "Training loss: 0.2862068706980118\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 238----------\n",
      "Training loss: 0.2862536291090342\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 239----------\n",
      "Training loss: 0.28680122930269975\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 240----------\n",
      "Training loss: 0.28627725690603256\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 241----------\n",
      "Training loss: 0.2895270586013794\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 242----------\n",
      "Training loss: 0.28797894143141234\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 243----------\n",
      "Training loss: 0.297084257293206\n",
      "Training error is 25.0%\n",
      "Testing error is 0.4117647058823529%\n",
      "----------Epoch 244----------\n",
      "Training loss: 0.28468473026385677\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 245----------\n",
      "Training loss: 0.28733275601497066\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 246----------\n",
      "Training loss: 0.29212249185030276\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 247----------\n",
      "Training loss: 0.2938416806551126\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 248----------\n",
      "Training loss: 0.2866924349218607\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 249----------\n",
      "Training loss: 0.29225243914585847\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 250----------\n",
      "Training loss: 0.2856699279867686\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 251----------\n",
      "Training loss: 0.29372400217331374\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 252----------\n",
      "Training loss: 0.2899918504632436\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 253----------\n",
      "Training loss: 0.28381877679091233\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 254----------\n",
      "Training loss: 0.28803890026532686\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 255----------\n",
      "Training loss: 0.2849543856886717\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 256----------\n",
      "Training loss: 0.29730694798322826\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 257----------\n",
      "Training loss: 0.2998405654843037\n",
      "Training error is 37.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 258----------\n",
      "Training loss: 0.29003438582787144\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 259----------\n",
      "Training loss: 0.2878491265269426\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 260----------\n",
      "Training loss: 0.2881268641123405\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 261----------\n",
      "Training loss: 0.2803501721758109\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 262----------\n",
      "Training loss: 0.2943056827554336\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 263----------\n",
      "Training loss: 0.2812679478755364\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 264----------\n",
      "Training loss: 0.28641949823269475\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 265----------\n",
      "Training loss: 0.2845776298871407\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 266----------\n",
      "Training loss: 0.28347074584319043\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 267----------\n",
      "Training loss: 0.28211440260593706\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 268----------\n",
      "Training loss: 0.293972610281064\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 269----------\n",
      "Training loss: 0.2853457824541972\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 270----------\n",
      "Training loss: 0.2758644661651208\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 271----------\n",
      "Training loss: 0.28881678730249405\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 272----------\n",
      "Training loss: 0.2800761673312921\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 273----------\n",
      "Training loss: 0.28184679723702943\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 274----------\n",
      "Training loss: 0.284880841580721\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 275----------\n",
      "Training loss: 0.2766741161736158\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 276----------\n",
      "Training loss: 0.28687505366710514\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 277----------\n",
      "Training loss: 0.28631498091495955\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 278----------\n",
      "Training loss: 0.274144936639529\n",
      "Training error is 0.0%\n",
      "Testing error is 0.47058823529411764%\n",
      "----------Epoch 279----------\n",
      "Training loss: 0.27929042795529735\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 280----------\n",
      "Training loss: 0.27491887314961505\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 281----------\n",
      "Training loss: 0.2779795028842412\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 282----------\n",
      "Training loss: 0.29097048995586544\n",
      "Training error is 50.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 283----------\n",
      "Training loss: 0.27658286232214707\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 284----------\n",
      "Training loss: 0.2793083953169676\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 285----------\n",
      "Training loss: 0.2874015581149321\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 286----------\n",
      "Training loss: 0.27443567032997424\n",
      "Training error is 0.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 287----------\n",
      "Training loss: 0.2725292644821681\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 288----------\n",
      "Training loss: 0.2775001021531912\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 289----------\n",
      "Training loss: 0.2712918141713509\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 290----------\n",
      "Training loss: 0.2743372762432465\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 291----------\n",
      "Training loss: 0.2703936455341486\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 292----------\n",
      "Training loss: 0.2690947742129748\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 293----------\n",
      "Training loss: 0.27690923099334425\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 294----------\n",
      "Training loss: 0.2835178558643048\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 295----------\n",
      "Training loss: 0.27248532611590165\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 296----------\n",
      "Training loss: 0.2722944582884128\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 297----------\n",
      "Training loss: 0.2689451861840028\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 298----------\n",
      "Training loss: 0.2704774903563353\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 299----------\n",
      "Training loss: 0.29205576559671986\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 300----------\n",
      "Training loss: 0.28678631495970947\n",
      "Training error is 37.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 301----------\n",
      "Training loss: 0.27233560326007694\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 302----------\n",
      "Training loss: 0.2708323769844495\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 303----------\n",
      "Training loss: 0.27458539719765\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 304----------\n",
      "Training loss: 0.27591722458601\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 305----------\n",
      "Training loss: 0.2681361029927547\n",
      "Training error is 0.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 306----------\n",
      "Training loss: 0.2666224338687383\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 307----------\n",
      "Training loss: 0.26594788466508573\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 308----------\n",
      "Training loss: 0.2652314262321362\n",
      "Training error is 0.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 309----------\n",
      "Training loss: 0.27170333667443347\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 310----------\n",
      "Training loss: 0.27272380544589114\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 311----------\n",
      "Training loss: 0.27957813556377703\n",
      "Training error is 37.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 312----------\n",
      "Training loss: 0.2653594948351383\n",
      "Training error is 0.0%\n",
      "Testing error is 0.4117647058823529%\n",
      "----------Epoch 313----------\n",
      "Training loss: 0.26491875373400176\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 314----------\n",
      "Training loss: 0.26369112801666444\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 315----------\n",
      "Training loss: 0.2676131238157933\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 316----------\n",
      "Training loss: 0.26362987607717514\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 317----------\n",
      "Training loss: 0.26671788612237346\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 318----------\n",
      "Training loss: 0.2698361214536887\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 319----------\n",
      "Training loss: 0.26107791109153855\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 320----------\n",
      "Training loss: 0.268724218584024\n",
      "Training error is 25.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 321----------\n",
      "Training loss: 0.27693493549640363\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 322----------\n",
      "Training loss: 0.27040552691771436\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 323----------\n",
      "Training loss: 0.26600508907666576\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 324----------\n",
      "Training loss: 0.2642687513278081\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 325----------\n",
      "Training loss: 0.26594847784592557\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 326----------\n",
      "Training loss: 0.2616394443007616\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 327----------\n",
      "Training loss: 0.2641071929381444\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 328----------\n",
      "Training loss: 0.2719871671153949\n",
      "Training error is 37.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 329----------\n",
      "Training loss: 0.2626411026486984\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 330----------\n",
      "Training loss: 0.2786827345307057\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 331----------\n",
      "Training loss: 0.275992472584431\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 332----------\n",
      "Training loss: 0.270384261241326\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 333----------\n",
      "Training loss: 0.26731661363289905\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 334----------\n",
      "Training loss: 0.2651855673354406\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 335----------\n",
      "Training loss: 0.2601824583342442\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 336----------\n",
      "Training loss: 0.2720749544409605\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 337----------\n",
      "Training loss: 0.2719164244257487\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 338----------\n",
      "Training loss: 0.2616844011040834\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 339----------\n",
      "Training loss: 0.2631764870423537\n",
      "Training error is 12.5%\n",
      "Testing error is 0.4117647058823529%\n",
      "----------Epoch 340----------\n",
      "Training loss: 0.26301891471330935\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 341----------\n",
      "Training loss: 0.2671075199659054\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 342----------\n",
      "Training loss: 0.2820398354759583\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 343----------\n",
      "Training loss: 0.2779339878604962\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 344----------\n",
      "Training loss: 0.26577993023854035\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 345----------\n",
      "Training loss: 0.25841799550331557\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 346----------\n",
      "Training loss: 0.2538269539005481\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 347----------\n",
      "Training loss: 0.2631424178297703\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 348----------\n",
      "Training loss: 0.2553262080137546\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 349----------\n",
      "Training loss: 0.2690863213860072\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 350----------\n",
      "Training loss: 0.26269942178176\n",
      "Training error is 12.5%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 351----------\n",
      "Training loss: 0.26098427692284953\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 352----------\n",
      "Training loss: 0.26186409478004163\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 353----------\n",
      "Training loss: 0.2595631932983032\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 354----------\n",
      "Training loss: 0.25657429775366414\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 355----------\n",
      "Training loss: 0.2503050478318563\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 356----------\n",
      "Training loss: 0.2652430889698175\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 357----------\n",
      "Training loss: 0.2604579392534036\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 358----------\n",
      "Training loss: 0.26821534908734834\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 359----------\n",
      "Training loss: 0.2558780782497846\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 360----------\n",
      "Training loss: 0.25099212174805313\n",
      "Training error is 0.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 361----------\n",
      "Training loss: 0.2641855959708874\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 362----------\n",
      "Training loss: 0.2548141055382215\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 363----------\n",
      "Training loss: 0.25604794117120594\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 364----------\n",
      "Training loss: 0.2558591721149591\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 365----------\n",
      "Training loss: 0.255408824636386\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 366----------\n",
      "Training loss: 0.265998145708671\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 367----------\n",
      "Training loss: 0.2545866198264636\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 368----------\n",
      "Training loss: 0.2579899166639035\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 369----------\n",
      "Training loss: 0.24897017501867735\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 370----------\n",
      "Training loss: 0.25531212756266963\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 371----------\n",
      "Training loss: 0.2501696338877082\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 372----------\n",
      "Training loss: 0.2518032474013475\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 373----------\n",
      "Training loss: 0.2540466281083914\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 374----------\n",
      "Training loss: 0.25025130579104793\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 375----------\n",
      "Training loss: 0.25007693469524384\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 376----------\n",
      "Training loss: 0.24964616562311465\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 377----------\n",
      "Training loss: 0.25334541155741763\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 378----------\n",
      "Training loss: 0.256645245047716\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 379----------\n",
      "Training loss: 0.2490679180392852\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 380----------\n",
      "Training loss: 0.255354296702605\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 381----------\n",
      "Training loss: 0.2484817519210852\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 382----------\n",
      "Training loss: 0.24868075721538985\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 383----------\n",
      "Training loss: 0.2522176985557263\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 384----------\n",
      "Training loss: 0.2565836774615141\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 385----------\n",
      "Training loss: 0.2456855854162803\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 386----------\n",
      "Training loss: 0.24282208185356396\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 387----------\n",
      "Training loss: 0.24727531522512436\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 388----------\n",
      "Training loss: 0.2610938944495641\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 389----------\n",
      "Training loss: 0.24841258216362733\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 390----------\n",
      "Training loss: 0.24697492328973916\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 391----------\n",
      "Training loss: 0.25614175859552163\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 392----------\n",
      "Training loss: 0.2512010353115889\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 393----------\n",
      "Training loss: 0.24835462180467752\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 394----------\n",
      "Training loss: 0.25564861526856053\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 395----------\n",
      "Training loss: 0.2506567302804727\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 396----------\n",
      "Training loss: 0.24628505024772424\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 397----------\n",
      "Training loss: 0.24872774516160673\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 398----------\n",
      "Training loss: 0.24332246098380822\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 399----------\n",
      "Training loss: 0.24473216671210068\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 400----------\n",
      "Training loss: 0.24663139650454888\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 401----------\n",
      "Training loss: 0.24976073004878485\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 402----------\n",
      "Training loss: 0.24701589460556322\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 403----------\n",
      "Training loss: 0.2506840183184697\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 404----------\n",
      "Training loss: 0.2467643446647204\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 405----------\n",
      "Training loss: 0.24927448022824067\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 406----------\n",
      "Training loss: 0.25585231471520203\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 407----------\n",
      "Training loss: 0.24561946036723944\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 408----------\n",
      "Training loss: 0.2501848775606889\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 409----------\n",
      "Training loss: 0.2454956340102049\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 410----------\n",
      "Training loss: 0.25204780239325303\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 411----------\n",
      "Training loss: 0.23894660318127045\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 412----------\n",
      "Training loss: 0.24356006945555025\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 413----------\n",
      "Training loss: 0.24197603819461969\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 414----------\n",
      "Training loss: 0.24287504975039226\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 415----------\n",
      "Training loss: 0.24286829680204391\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 416----------\n",
      "Training loss: 0.2386394667510803\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 417----------\n",
      "Training loss: 0.24324638281877226\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 418----------\n",
      "Training loss: 0.24761017125386459\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 419----------\n",
      "Training loss: 0.2455226698747048\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 420----------\n",
      "Training loss: 0.24988258744661623\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 421----------\n",
      "Training loss: 0.23776919939197028\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 422----------\n",
      "Training loss: 0.24088553396555093\n",
      "Training error is 0.0%\n",
      "Testing error is 0.4117647058823529%\n",
      "----------Epoch 423----------\n",
      "Training loss: 0.24482306322226158\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 424----------\n",
      "Training loss: 0.24074696290951508\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 425----------\n",
      "Training loss: 0.23901704182991615\n",
      "Training error is 12.5%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 426----------\n",
      "Training loss: 0.23623872290437037\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 427----------\n",
      "Training loss: 0.23543212763392007\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 428----------\n",
      "Training loss: 0.2401671730555021\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 429----------\n",
      "Training loss: 0.24109095621567506\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 430----------\n",
      "Training loss: 0.24260077711481315\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 431----------\n",
      "Training loss: 0.23857300270062226\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 432----------\n",
      "Training loss: 0.25105861459787077\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 433----------\n",
      "Training loss: 0.23905734517253363\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 434----------\n",
      "Training loss: 0.23600109838522398\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 435----------\n",
      "Training loss: 0.24361127271102026\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 436----------\n",
      "Training loss: 0.2486769866484862\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 437----------\n",
      "Training loss: 0.234505156484934\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 438----------\n",
      "Training loss: 0.23588232294871256\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 439----------\n",
      "Training loss: 0.24080633085507613\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 440----------\n",
      "Training loss: 0.24110545877080697\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 441----------\n",
      "Training loss: 0.23690426722168922\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 442----------\n",
      "Training loss: 0.23849441970770174\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 443----------\n",
      "Training loss: 0.2502125312502568\n",
      "Training error is 25.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 444----------\n",
      "Training loss: 0.25068837862748367\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 445----------\n",
      "Training loss: 0.23092385954581773\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 446----------\n",
      "Training loss: 0.23958240048243448\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 447----------\n",
      "Training loss: 0.23815575070106065\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 448----------\n",
      "Training loss: 0.23561172932386398\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 449----------\n",
      "Training loss: 0.23885960418444413\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 450----------\n",
      "Training loss: 0.23559389206079337\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 451----------\n",
      "Training loss: 0.23996635469106528\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 452----------\n",
      "Training loss: 0.24687955070000428\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 453----------\n",
      "Training loss: 0.2355997092448748\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 454----------\n",
      "Training loss: 0.23219688867147154\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 455----------\n",
      "Training loss: 0.23259597902114576\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 456----------\n",
      "Training loss: 0.23275229392143396\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 457----------\n",
      "Training loss: 0.23177526275125834\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 458----------\n",
      "Training loss: 0.2300887073461826\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 459----------\n",
      "Training loss: 0.23410292284993026\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 460----------\n",
      "Training loss: 0.23815355793787882\n",
      "Training error is 12.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 461----------\n",
      "Training loss: 0.2398785501718521\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 462----------\n",
      "Training loss: 0.23184956552890632\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 463----------\n",
      "Training loss: 0.2429695840065296\n",
      "Training error is 25.0%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 464----------\n",
      "Training loss: 0.2410907192298999\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 465----------\n",
      "Training loss: 0.23946035022918993\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 466----------\n",
      "Training loss: 0.22802250173229438\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 467----------\n",
      "Training loss: 0.24570608425598878\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 468----------\n",
      "Training loss: 0.24784726534898466\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 469----------\n",
      "Training loss: 0.23209111335185859\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 470----------\n",
      "Training loss: 0.2469713779596182\n",
      "Training error is 37.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 471----------\n",
      "Training loss: 0.22907291782590058\n",
      "Training error is 12.5%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 472----------\n",
      "Training loss: 0.22601216257764742\n",
      "Training error is 0.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 473----------\n",
      "Training loss: 0.23515875179034013\n",
      "Training error is 25.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 474----------\n",
      "Training loss: 0.2478298768401146\n",
      "Training error is 37.5%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 475----------\n",
      "Training loss: 0.22652884830649084\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 476----------\n",
      "Training loss: 0.24465266787088835\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 477----------\n",
      "Training loss: 0.2393551027545562\n",
      "Training error is 25.0%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 478----------\n",
      "Training loss: 0.227216371263449\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 479----------\n",
      "Training loss: 0.24463871465279505\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 480----------\n",
      "Training loss: 0.23014771393858469\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 481----------\n",
      "Training loss: 0.23455376292650515\n",
      "Training error is 25.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 482----------\n",
      "Training loss: 0.2303173289849208\n",
      "Training error is 12.5%\n",
      "Testing error is 0.3529411764705882%\n",
      "----------Epoch 483----------\n",
      "Training loss: 0.23231156858114096\n",
      "Training error is 12.5%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 484----------\n",
      "Training loss: 0.2341838335761657\n",
      "Training error is 37.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 485----------\n",
      "Training loss: 0.2286850162423574\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 486----------\n",
      "Training loss: 0.2285010264470027\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 487----------\n",
      "Training loss: 0.22445116134790274\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 488----------\n",
      "Training loss: 0.2223037647513243\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 489----------\n",
      "Training loss: 0.24251104891300201\n",
      "Training error is 25.0%\n",
      "Testing error is 0.0%\n",
      "----------Epoch 490----------\n",
      "Training loss: 0.2264010370350801\n",
      "Training error is 12.5%\n",
      "Testing error is 0.11764705882352944%\n",
      "----------Epoch 491----------\n",
      "Training loss: 0.22504513882673705\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 492----------\n",
      "Training loss: 0.22278278664900705\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 493----------\n",
      "Training loss: 0.22173961968376085\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 494----------\n",
      "Training loss: 0.22732997800295168\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 495----------\n",
      "Training loss: 0.2381359447653477\n",
      "Training error is 25.0%\n",
      "Testing error is 0.2941176470588235%\n",
      "----------Epoch 496----------\n",
      "Training loss: 0.2260500146792485\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 497----------\n",
      "Training loss: 0.22432742583063933\n",
      "Training error is 0.0%\n",
      "Testing error is 0.23529411764705888%\n",
      "----------Epoch 498----------\n",
      "Training loss: 0.22476368999251953\n",
      "Training error is 0.0%\n",
      "Testing error is 0.05882352941176472%\n",
      "----------Epoch 499----------\n",
      "Training loss: 0.22711176683123296\n",
      "Training error is 0.0%\n",
      "Testing error is 0.17647058823529416%\n",
      "----------Epoch 500----------\n",
      "Training loss: 0.22095767179360756\n",
      "Training error is 0.0%\n",
      "Testing error is 0.2941176470588235%\n"
     ]
    }
   ],
   "source": [
    "model1 = feedforward_NN(num_input=12, num_output=1, num_layer=1, num_neuron=20, activation='relu')\n",
    "\n",
    "optimizer = torch.optim.Adam(model1.parameters(),lr=learning_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "trained_model1 = train_NN(model1, epochs, trainloader, optimizer, criterion, activation=\"relu\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
